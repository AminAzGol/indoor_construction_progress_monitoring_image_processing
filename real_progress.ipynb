{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_range = (3840,3999)\n",
    "im_path = \"data/real_progress_marked/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    display(PIL.Image.fromarray(img))\n",
    "def show_plt(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = os.listdir(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh(gray, thrsh):\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    ret, th = cv2.threshold(blur, thrsh, 255, cv2.THRESH_BINARY)\n",
    "    return th\n",
    "def saveImg(directory,i, img):\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    path = directory+str(i)+\".jpg\"\n",
    "    cv2.imwrite(path,img)\n",
    "    return True\n",
    "def getPerspectiveTransform(points, col_dis):\n",
    "    output_size = 4000\n",
    "    startx = output_size / 2 - col_dis[0]/2\n",
    "    endx = output_size / 2 + col_dis[0]/2\n",
    "\n",
    "    starty = output_size / 2 - col_dis[1]/2\n",
    "    endy = output_size / 2 + col_dis[1]/2\n",
    "\n",
    "    targetPoints = [(startx, starty), (endx, starty), (startx, endy), (endx, endy)]\n",
    "\n",
    "    empty_img = np.ones((output_size, output_size), dtype=np.uint8)\n",
    "    \n",
    "    pst1 = np.float32(points)\n",
    "    pst2 = np.float32(targetPoints)\n",
    "    '''\n",
    "    A perspective transform relates two points in the following manner:  \n",
    "    [x']   [m00 m01 m02] [x]\n",
    "    [y'] = [m10 m11 m12] [y]\n",
    "    [1]    [m20 m21 m22] [1]\n",
    "    Where (x,y) are the original 2D point coordinates, and (x', y') are the transformed coordinates.\n",
    "    '''\n",
    "    matrix = cv2.getPerspectiveTransform(pst1, pst2)\n",
    "    return matrix\n",
    "def find_contours(bimg, min_size):\n",
    "    c, h = cv2.findContours(bimg, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    c = [a for a in c if a.shape[0] > min_size]\n",
    "    return c\n",
    "def contours_area_count(c):\n",
    "    sum = 0\n",
    "    for i in c:\n",
    "        area = cv2.contourArea(i)\n",
    "        sum += area\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840j.jpg\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-221706d7e13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcsvPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'export_dataframe_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-221706d7e13c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mth_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_contours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0marea_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontours_area_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprv_area\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0marea_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1ee35255133b>\u001b[0m in \u001b[0;36mfind_contours\u001b[0;34m(bimg, min_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_contours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    frame_numbers = []\n",
    "    areas = []\n",
    "    pathArr = []\n",
    "    method = 'REAL'\n",
    "    prv_area = 0\n",
    "    for img_file_name in images_list:\n",
    "        print(img_file_name)\n",
    "        img = cv2.imread(im_path+img_file_name)\n",
    "        red_channel = img[:,:,2]\n",
    "        th = thresh(red_channel, 250)\n",
    "        points = [(791, 534), (1071, 530), (583, 672), (1289, 658)]\n",
    "        matrix = getPerspectiveTransform(points, (850,900))\n",
    "        output_size =4000\n",
    "        th_transformed = cv2.warpPerspective(th, matrix, (output_size, output_size))\n",
    "        c = find_contours(th_transformed, 300)\n",
    "        area_count = contours_area_count(c)\n",
    "        if prv_area > area_count:\n",
    "            area_count = prv_area\n",
    "        prv_area = area_count\n",
    "        areas.append(area_count)\n",
    "        i = img_file_name.split('j')[0]\n",
    "        frame_numbers.append(i)\n",
    "\n",
    "        th_rgb = cv2.cvtColor(th_transformed, cv2.COLOR_BAYER_BG2BGR)\n",
    "        cv2.drawContours(th_rgb, c, -1, (0, 255, 0), 2)\n",
    "        threshImgsDir = 'flow_results/'+method+\"/thresh/\"\n",
    "        saveImg(threshImgsDir, i, th_rgb)\n",
    "        pathArr.append(str(i)+'.jpg')\n",
    "    \n",
    "    df = pd.DataFrame({'Frame':frame_numbers, 'Area':areas, 'path': pathArr})\n",
    "    csvPath = 'export_dataframe_'+method+'.csv'\n",
    "    df.to_csv(csvPath, index=False)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
